{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Détecter les faux billets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Votre société de consulting informatique vous propose une nouvelle mission au ministère de l'Intérieur, dans le cadre de la lutte contre la criminalité organisée, à l'Office central pour la répression du faux monnayage. Votre mission si vous l'acceptez : **créer un algorithme de détection de faux billets**.  \n",
    "Vous vous voyez déjà en grand justicier combattant sans relâche la criminalité organisée en pianotant à mains de maître votre ordinateur, pour façonner ce fabuleux algorithme  qui traquera la moindre fraude et permettra de mettre à jour les réseaux secrets de faux-monnayeurs ! La classe, non ?  \n",
    "Bon, si on retombait les pieds sur terre ? Travailler pour la police judiciaire, c'est bien, mais vous allez devoir faire appel à vos connaissances en statistiques, alors on y va !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importation des modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.collections import LineCollection\n",
    "import matplotlib.mlab as mlab\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "from sklearn import decomposition, preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans \n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import balanced_accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_genuine</th>\n",
       "      <th>diagonal</th>\n",
       "      <th>height_left</th>\n",
       "      <th>height_right</th>\n",
       "      <th>margin_low</th>\n",
       "      <th>margin_up</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>171.81</td>\n",
       "      <td>104.86</td>\n",
       "      <td>104.95</td>\n",
       "      <td>4.52</td>\n",
       "      <td>2.89</td>\n",
       "      <td>112.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>171.67</td>\n",
       "      <td>103.74</td>\n",
       "      <td>103.70</td>\n",
       "      <td>4.01</td>\n",
       "      <td>2.87</td>\n",
       "      <td>113.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>171.83</td>\n",
       "      <td>103.76</td>\n",
       "      <td>103.76</td>\n",
       "      <td>4.40</td>\n",
       "      <td>2.88</td>\n",
       "      <td>113.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>171.80</td>\n",
       "      <td>103.78</td>\n",
       "      <td>103.65</td>\n",
       "      <td>3.73</td>\n",
       "      <td>3.12</td>\n",
       "      <td>113.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>True</td>\n",
       "      <td>172.05</td>\n",
       "      <td>103.70</td>\n",
       "      <td>103.75</td>\n",
       "      <td>5.04</td>\n",
       "      <td>2.27</td>\n",
       "      <td>113.55</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_genuine  diagonal  height_left  height_right  margin_low  margin_up  \\\n",
       "0        True    171.81       104.86        104.95        4.52       2.89   \n",
       "1        True    171.67       103.74        103.70        4.01       2.87   \n",
       "2        True    171.83       103.76        103.76        4.40       2.88   \n",
       "3        True    171.80       103.78        103.65        3.73       3.12   \n",
       "4        True    172.05       103.70        103.75        5.04       2.27   \n",
       "\n",
       "   length  \n",
       "0  112.83  \n",
       "1  113.29  \n",
       "2  113.84  \n",
       "3  113.63  \n",
       "4  113.55  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('/Users/anissa/P6/notes.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La _longueur du billet_ (en mm) : **length**  \n",
    "La _hauteur du billet_ (mesurée sur le côté gauche, en mm) : **height_left**  \n",
    "La _hauteur du billet_ (mesurée sur le côté droit, en mm) : **height_right**  \n",
    "La _marge entre le bord supérieur_ du billet et l'image de celui-ci (en mm) : **margin_up**  \n",
    "La _marge entre le bord inférieur_ du billet et l'image de celui-ci (en mm) : **margin_low**  \n",
    "La _diagonale du billet_ (en mm) : **diagonal**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(170, 7)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le df est composé de **170 billets** et de **7 variables** : \n",
    "* 6 variables quantititaves : _diagonal, height_left, height_right, margin_low, margin_up et length_\n",
    "* 1 qualitative : *is_genuine*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valeurs manquantes :\n",
      "is_genuine      0\n",
      "diagonal        0\n",
      "height_left     0\n",
      "height_right    0\n",
      "margin_low      0\n",
      "margin_up       0\n",
      "length          0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Recherche des valeurs manquantes\n",
    "print('Valeurs manquantes :\\n' + str(data.isnull().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valeurs dupliquées :  0\n"
     ]
    }
   ],
   "source": [
    "# Recherche des valeurs dupliquées\n",
    "print('Valeurs dupliquées : ', data.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Analyses univariées et bivariées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_genuine               False       True \n",
      "diagonal     count   70.000000  100.000000\n",
      "             mean   171.889857  171.976100\n",
      "             std      0.297426    0.307981\n",
      "             min    171.380000  171.040000\n",
      "             25%    171.682500  171.790000\n",
      "             50%    171.875000  172.005000\n",
      "             75%    172.047500  172.162500\n",
      "             max    173.010000  172.750000\n",
      "height_left  count   70.000000  100.000000\n",
      "             mean   104.230429  103.951500\n",
      "             std      0.213130    0.296251\n",
      "             min    103.780000  103.230000\n",
      "             25%    104.082500  103.740000\n",
      "             50%    104.215000  103.915000\n",
      "             75%    104.377500  104.145000\n",
      "             max    104.720000  104.860000\n",
      "height_right count   70.000000  100.000000\n",
      "             mean   104.145571  103.775900\n",
      "             std      0.253152    0.292406\n",
      "             min    103.440000  103.140000\n",
      "             25%    103.982500  103.557500\n",
      "             50%    104.170000  103.760000\n",
      "             75%    104.280000  103.972500\n",
      "             max    104.860000  104.950000\n",
      "margin_low   count   70.000000  100.000000\n",
      "             mean     5.281571    4.143500\n",
      "             std      0.540846    0.314509\n",
      "             min      3.820000    3.540000\n",
      "             25%      4.952500    3.900000\n",
      "             50%      5.265000    4.080000\n",
      "             75%      5.702500    4.382500\n",
      "             max      6.280000    5.040000\n",
      "margin_up    count   70.000000  100.000000\n",
      "             mean     3.334571    3.055500\n",
      "             std      0.185102    0.197726\n",
      "             min      2.980000    2.270000\n",
      "             25%      3.185000    2.937500\n",
      "             50%      3.335000    3.070000\n",
      "             75%      3.450000    3.192500\n",
      "             max      3.680000    3.530000\n",
      "length       count   70.000000  100.000000\n",
      "             mean   111.660714  113.207200\n",
      "             std      0.676931    0.380476\n",
      "             min    109.970000  111.760000\n",
      "             25%    111.270000  112.995000\n",
      "             50%    111.765000  113.210000\n",
      "             75%    111.985000  113.505000\n",
      "             max    113.640000  113.980000\n"
     ]
    }
   ],
   "source": [
    "description = data.groupby('is_genuine').describe().T\n",
    "print(description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notre échantillon contient 170 billets : 100 vrais billets et 70 faux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrice de corrélation\n",
    "sns.set_theme(style=\"white\")\n",
    "d = data\n",
    "corr = d.corr()\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "\n",
    "f, ax = plt.subplots(figsize=(25, 15))\n",
    "\n",
    "\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "\n",
    "\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True)\n",
    "plt.title(\"Matrice de corrélation\", size=15)\n",
    "plt.savefig('/Users/anissa/P6_matrice_correlation.jpg', dpi=1200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les variables \"length\" et \"margin_low\" sont les variables **les plus corrélées à la variable \"is_genuine\"**. Ce qui signifie que ce sont ces 2 variables qui sont les plus **importantes** lors de la détection de faux billets.  \n",
    "On constate une **très faible corrélation** entre \"margin_up\" et \"diagonal\".  \n",
    "Une **forte corrélation positive** est constatée entre \"height_right\" et \"height_left\" : les deux variables varient dans le même sens.   \n",
    "Une **corrélation négative** existe entre \"margin_low\" et \"length\" : lorsqu'une des variables augmente, l'autre diminue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On constate bien une différence significative entre les données des vrais et des faux billets.  \n",
    "La variable \"length\" montre que la taille des faux billets est inférieure à celle des vrais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison entre les vrais et faux billets (violinplot)\n",
    "sns.set_palette('Spectral')\n",
    "medianprops = dict(linewidth=2, color='grey')\n",
    "meanprops={\"marker\":\"o\",\n",
    "                       \"markerfacecolor\":\"yellow\", \n",
    "                       \"markeredgecolor\":\"yellow\",\n",
    "                      \"markersize\":\"5\"}\n",
    "\n",
    "for variable in data.columns[1:7]:\n",
    "    sns_plot=sns.violinplot(\n",
    "        x = data[variable], \n",
    "        y = data.is_genuine,\n",
    "        orient = \"h\",\n",
    "        medianprops = medianprops, showmeans=True, meanprops=meanprops,\n",
    "        width = .5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison entre les dimensions des vrais et faux billets (boxplot)\n",
    "medianprops = dict(linewidth=2, color='grey')\n",
    "meanprops={\"marker\":\"o\",\n",
    "                       \"markerfacecolor\":\"yellow\", \n",
    "                       \"markeredgecolor\":\"yellow\",\n",
    "                      \"markersize\":\"5\"}\n",
    "\n",
    "for variable in data.columns[1:7]:\n",
    "    sns_plot=sns.boxplot(\n",
    "        x = data[variable], \n",
    "        y = data.is_genuine,\n",
    "        orient = \"h\",\n",
    "        medianprops = medianprops, showmeans=True, meanprops=meanprops,\n",
    "        width = .5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On constate ici, comme pour la matrice de corrélation, que les valeurs les plus discriminantes sont **\"length\"** et **\"margin_low\"** : différences importantes entre les dimensions et données plus hétérogènes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Analyse en Composantes Principales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Source :_ http://eric.univ-lyon2.fr/~ricco/tanagra/fichiers/fr_Tanagra_ACP_Python.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramétrage de l'environnement\n",
    "\n",
    "def display_circles(pcs, n_comp, pca, axis_ranks, labels=None, label_rotation=0, lims=None):\n",
    "    for d1, d2 in axis_ranks: # On affiche les 3 premiers plans factoriels, donc les 6 premières composantes\n",
    "        if d2 < n_comp:\n",
    "\n",
    "            # initialisation de la figure\n",
    "            fig, ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "            # détermination des limites du graphique\n",
    "            if lims is not None :\n",
    "                xmin, xmax, ymin, ymax = lims\n",
    "            elif pcs.shape[1] < 30 :\n",
    "                xmin, xmax, ymin, ymax = -1, 1, -1, 1\n",
    "            else :\n",
    "                xmin, xmax, ymin, ymax = min(pcs[d1,:]), max(pcs[d1,:]), min(pcs[d2,:]), max(pcs[d2,:])\n",
    "\n",
    "            # affichage des flèches\n",
    "            # s'il y a plus de 30 flèches, on n'affiche pas le triangle à leur extrémité\n",
    "            if pcs.shape[1] < 30 :\n",
    "                plt.quiver(np.zeros(pcs.shape[1]), np.zeros(pcs.shape[1]),\n",
    "                   pcs[d1,:], pcs[d2,:], \n",
    "                   angles='xy', scale_units='xy', scale=1, color=\"grey\")\n",
    "                # (voir la doc : https://matplotlib.org/api/_as_gen/matplotlib.pyplot.quiver.html)\n",
    "            else:\n",
    "                lines = [[[0,0],[x,y]] for x,y in pcs[[d1,d2]].T]\n",
    "                ax.add_collection(LineCollection(lines, axes=ax, alpha=.1, color='black'))\n",
    "            \n",
    "            # affichage des noms des variables  \n",
    "            if labels is not None:  \n",
    "                for i,(x, y) in enumerate(pcs[[d1,d2]].T):\n",
    "                    if x >= xmin and x <= xmax and y >= ymin and y <= ymax :\n",
    "                      plt.text(x, y, labels[i], fontsize='14', ha='center', va='center', rotation=label_rotation, color=\"blue\", alpha=0.5)\n",
    "            \n",
    "            # affichage du cercle\n",
    "            an = np.linspace(0, 2 * np.pi, 100)  # Add a unit circle for scale\n",
    "            plt.plot(np.cos(an), np.sin(an))\n",
    "            plt.axis('equal')\n",
    "\n",
    "            # définition des limites du graphique\n",
    "            plt.xlim(xmin, xmax)\n",
    "            plt.ylim(ymin, ymax)\n",
    "        \n",
    "            # affichage des lignes horizontales et verticales\n",
    "            plt.plot([-1, 1], [0, 0], color='grey', ls='--')\n",
    "            plt.plot([0, 0], [-1, 1], color='grey', ls='--')\n",
    "\n",
    "            # nom des axes, avec le pourcentage d'inertie expliqué\n",
    "            plt.xlabel('F{} ({}%)'.format(d1+1, round(100*pca.explained_variance_ratio_[d1],1)))\n",
    "            plt.ylabel('F{} ({}%)'.format(d2+1, round(100*pca.explained_variance_ratio_[d2],1)))\n",
    "\n",
    "            plt.title(\"Cercle des corrélations (F{} et F{})\".format(d1+1, d2+1))\n",
    "            plt.show(block=False)\n",
    "        \n",
    "def display_factorial_planes(X_projected, n_comp, pca, axis_ranks, labels=None, alpha=1, illustrative_var=None):\n",
    "    for d1,d2 in axis_ranks:\n",
    "        if d2 < n_comp:\n",
    " \n",
    "            # initialisation de la figure       \n",
    "            fig = plt.figure(figsize=(10,10))\n",
    "        \n",
    "            # affichage des points\n",
    "            if illustrative_var is None:\n",
    "                plt.scatter(X_projected[:, d1], X_projected[:, d2], alpha=alpha)\n",
    "            else:\n",
    "                illustrative_var = np.array(illustrative_var)\n",
    "                for value in np.unique(illustrative_var):\n",
    "                    selected = np.where(illustrative_var == value)\n",
    "                    plt.scatter(X_projected[selected, d1], X_projected[selected, d2], alpha=alpha, label=value)\n",
    "                plt.legend()\n",
    "\n",
    "            # affichage des labels des points\n",
    "            if labels is not None:\n",
    "                for i,(x,y) in enumerate(X_projected[:,[d1,d2]]):\n",
    "                    plt.text(x, y, labels[i],\n",
    "                              fontsize='14', ha='center',va='center') \n",
    "                \n",
    "            # détermination des limites du graphique\n",
    "            boundary = np.max(np.abs(X_projected[:, [d1,d2]])) * 1.1\n",
    "            plt.xlim([-boundary,boundary])\n",
    "            plt.ylim([-boundary,boundary])\n",
    "        \n",
    "            # affichage des lignes horizontales et verticales\n",
    "            plt.plot([-100, 100], [0, 0], color='grey', ls='--')\n",
    "            plt.plot([0, 0], [-100, 100], color='grey', ls='--')\n",
    "\n",
    "            # nom des axes, avec le pourcentage d'inertie expliqué\n",
    "            plt.xlabel('F{} ({}%)'.format(d1+1, round(100*pca.explained_variance_ratio_[d1],1)))\n",
    "            plt.ylabel('F{} ({}%)'.format(d2+1, round(100*pca.explained_variance_ratio_[d2],1)))\n",
    "\n",
    "            plt.title(\"Projection des individus (sur F{} et F{})\".format(d1+1, d2+1))\n",
    "            plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_acp = data.drop(columns='is_genuine') # on supprime la variable qualitative pour l'ACP\n",
    "\n",
    "print(data_acp.shape) # dimension de la matrice\n",
    "n = data_acp.shape[0] # nombre d'observations\n",
    "p = data_acp.shape[1] # nombre de variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "Z = sc.fit_transform(data_acp)\n",
    "\n",
    "print(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acp = PCA() # instanciation\n",
    "print(acp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coord = acp.fit_transform(Z)\n",
    "print(acp.n_components_) # nombre de composantes calculées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variance expliquée\n",
    "print(acp.explained_variance_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valeur corrigée\n",
    "eigval = (n-1)/n*acp.explained_variance_\n",
    "print(eigval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proportion de valeurs expliquées\n",
    "ratio = acp.explained_variance_ratio_ * 100\n",
    "print(acp.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eboulis des valeurs propres\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "scree = acp.explained_variance_ratio_*100\n",
    "\n",
    "plt.bar(np.arange(len(scree))+1, scree)\n",
    "plt.plot(np.arange(len(scree))+1, scree.cumsum(),marker='o', color='r')\n",
    "plt.xlabel(\"Rang de l'axe d'inertie\")\n",
    "plt.ylabel(\"Pourcentage d'inertie\")\n",
    "\n",
    "plt.title(\"Eboulis des valeurs propres\")\n",
    "\n",
    "plt.savefig('/Users/anissa/P6_eboulis.jpg', dpi=1200)\n",
    "plt.show(block=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le critère du Kaiser nous conduit à retenir les deux premiers axes. En effet le premier axe retient 47.4% de l’inertie totale quant à l’axe 2 retient tout de même 22% de l’inertie, ce qui n’est pas négligeable. Et qui conduit à un taux d’inertie expliquée de 69,4%, ce qui est un très bon résultat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contribution des individus dans l'inertie totale\n",
    "di = np.sum(Z**2,axis=1)\n",
    "print(pd.DataFrame({'ID':data_acp.index,'d_i':di}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qualité de représentation des individus - COS2\n",
    "cos2 = coord**2\n",
    "\n",
    "for j in range(p):\n",
    "  cos2[:,j] = cos2[:,j]/di\n",
    "\n",
    "qualite = pd.DataFrame({'id':data_acp.index,'COS2_F1':cos2[:,0],'COS2_F2':cos2[:,1]})\n",
    "\n",
    "print(qualite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(qualite.sort_values('COS2_F1', ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(qualite.sort_values('COS2_F2', ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vérifions la théorie - somme en ligne des cos2 = 1\n",
    "print(np.sum(cos2,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contribution aux axes\n",
    "ctr = coord**2\n",
    "for j in range(p):\n",
    "  ctr[:,j] = ctr[:,j]/(n*eigval[j]) # eigval = c. 40 (variance expliquée et corrigé)\n",
    "\n",
    "contribution = pd.DataFrame({'id':data_acp.index,'CTR_F1':ctr[:,0],'CTR_F2':ctr[:,1]})\n",
    "print(contribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(contribution.sort_values('CTR_F1', ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(contribution.sort_values('CTR_F2', ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifions la théorie\n",
    "print(np.sum(ctr,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Le champs components_ de l'objet ACP\n",
    "print(acp.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Racine carrée des valeurs propres\n",
    "sqrt_eigval = np.sqrt(eigval)\n",
    "\n",
    "# Corrélation des variables avec les axes\n",
    "corvar = np.zeros((p,p))\n",
    "for k in range(p):\n",
    "  corvar[:,k] = acp.components_[k,:] * sqrt_eigval[k]\n",
    "\n",
    "# Afficher la matrice des corrélations variables x facteurs    \n",
    "print(corvar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On affiche pour les deux premiers axes\n",
    "print(pd.DataFrame({'id':data_acp.columns,'COR_1':corvar[:,0],'COR_2':corvar[:,1]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_acp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cercle des corrélations\n",
    "pcs = acp.components_\n",
    "features = data_acp.columns\n",
    "display_circles(pcs, p, acp, [(0,1)], labels = np.array(features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sur F1, nous voyons les variables liées aux hauteurs et aux marges.  \n",
    "La variable \"length\" est corrélée à F2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Projection des individus\n",
    "display_factorial_planes(coord, p, acp, [(0,1)], illustrative_var=data['is_genuine'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contient 69,4% de l’information : on observe très distinctement les deux groupes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosinus carré des variables\n",
    "cos2var = corvar**2\n",
    "print(pd.DataFrame({'id':data_acp.columns,'COS2_1':cos2var[:,0],'COS2_2':cos2var[:,1]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contributions\n",
    "ctrvar = cos2var\n",
    "for k in range(p):\n",
    "  ctrvar[:,k] = ctrvar[:,k]/eigval[k]\n",
    "\n",
    "# On n'affiche que pour les deux premiers axes\n",
    "print(pd.DataFrame({'id':data_acp.columns,'CTR_1':ctrvar[:,0],'CTR_2':ctrvar[:,1]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV. Classification : k-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons séparer notre échantillon en 2 groupes : variances égales et intertie minime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.values\n",
    "\n",
    "km = KMeans(n_clusters=2)\n",
    "km.fit(X)\n",
    "clustersk = km.labels_\n",
    "\n",
    "data_clusterk = pd.DataFrame({'clusters_kmeans' : clustersk})\n",
    "data = data.join(data_clusterk)\n",
    "illustrative_var = data['clusters_kmeans']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description = data.groupby('clusters_kmeans').describe().transpose()\n",
    "print(description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description2 = data.groupby('is_genuine').describe().transpose()\n",
    "print(description2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moyennes des dimensions pour les 2 clusters\n",
    "data.groupby('clusters_kmeans').mean().drop(\"is_genuine\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moyennes des dimensions pour les 2 clusters (k-means)\n",
    "data.groupby('is_genuine').mean().drop(\"clusters_kmeans\",axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette comparaison permet de voir que le clustering a bien fonctionné : la fonction a crée deux groupes qui sont très proches des groupes vrais et faux. On réalise maintenant une matrice de confusion afin de vérifier la qualité de notre classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrice de confusion\n",
    "df_confusion = pd.crosstab(data.is_genuine,data.clusters_kmeans)\n",
    "df_confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Représentation graphique de la matrice de confusion : heat map\n",
    "sns.heatmap(df_confusion, annot=True)\n",
    "\n",
    "plt.title(\"Matrice de confusion\", size=15)\n",
    "plt.savefig('/Users/anissa/P6_matrice_confusion.jpg', dpi=1200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "69 vrais négatifs et 100 vrais positifs ainsi qu'1 faux négatif (négatif à tord).  \n",
    "Pour comparaison, en regardant la variable \"is_genuine\" de notre échantillon, il y a 100 vrais billets et 70 faux billets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Projection des individus\n",
    "display_factorial_planes(coord, p, acp, [(0,1)],\n",
    "                         illustrative_var=data.clusters_kmeans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V. Modélisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_sources :_ \n",
    "* https://www.youtube.com/watch?v=xYDgnjtVFgU\n",
    "* http://eric.univ-lyon2.fr/~ricco/tanagra/fichiers/fr_Tanagra_Python_Regression_Logistique.pdf\n",
    "* https://towardsdatascience.com/evaluating-machine-learning-classification-problems-in-python-5-1-metrics-that-matter-792c6faddf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables explicatives\n",
    "X = data[['length', 'height_left', 'height_right', 'margin_low', 'margin_up', 'diagonal']]\n",
    "\n",
    "# Variable à expliquer\n",
    "y = data.is_genuine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition aléatoire du jeu de données en 80% pour créer le modèle, 20% pour tester le modèle\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Régression logistique\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stockage de prédictions\n",
    "predictions = model.predict(X_test)\n",
    "print(predictions)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation du modèle\n",
    "print(classification_report(y_test, predictions))\n",
    "print(accuracy_score(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrice de confusion\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "\n",
    "# Assignation du nom des colonnes\n",
    "cm_df = pd.DataFrame(cm, \n",
    "            columns = ['Predicted Negative', 'Predicted Positive'],\n",
    "            index = ['Actual Negative', 'Actual Positive'])\n",
    "cm_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VI. Calculs manuels des indicateurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vrai positif (true positive)\n",
    "TP = cm_df.iloc[1,1]\n",
    "\n",
    "# Vrai négatif (true negative)\n",
    "TN = cm_df.iloc[0,0]\n",
    "\n",
    "# Faux négatif (false negative)\n",
    "FN = cm_df.iloc[0,1]\n",
    "\n",
    "# Faux positif (false positive)\n",
    "FP = cm_df.iloc[1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul de la sensibilité (taux de vrais positifs)\n",
    "conf_sensitivity = (TP / float(TP + FN))\n",
    "\n",
    "# Calcul de la spécificité (taux de vrais négatifs)\n",
    "conf_specificity = (TN / float(TN + FP))\n",
    "\n",
    "print(\"sensivity =\",conf_sensitivity)\n",
    "print(\"specificity =\",conf_specificity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul de la précision (accuracy score)\n",
    "conf_accuracy = (float (TP+TN) / float(TP + TN + FP + FN))\n",
    "conf_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moyenne entre la sensibilité et la spécificité (balanced accuracy)\n",
    "balanced_accuracy_score(y_test, predictions, sample_weight=None, adjusted=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VII. Test nouvel échantillon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sample = pd.read_csv('/Users/anissa/P6/example.csv')\n",
    "new_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Préparation des données\n",
    "new_predict = new_sample[['length', 'height_left', 'height_right', 'margin_low', 'margin_up', 'diagonal']]\n",
    "\n",
    "# Application du modèle\n",
    "predict = model.predict(new_predict)\n",
    "\n",
    "# Probabilités\n",
    "model.predict_proba(new_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordre de lecture des probabilités\n",
    "model.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul des probas d'affectaion sur l'ech. à prédire\n",
    "probas_ex = model.predict_proba(new_predict)\n",
    "\n",
    "new_predict['Probas_faux'] = probas_ex[:,0]\n",
    "new_predict['Probas_vrais'] = probas_ex[:,1]\n",
    "new_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajout du resultat et création du df\n",
    "prediction=pd.DataFrame({'id': new_sample.id,\n",
    "                        'probalité_true' : new_predict.Probas_vrais,\n",
    "                        'probalité_false' : new_predict.Probas_faux,\n",
    "                        'prédiction' : predict})\n",
    "\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notre échantillon contient donc 3 faux billets et 2 vrais."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
